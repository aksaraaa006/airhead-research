

package edu.ucla.sspace.matrix;

/**
 * @author Keith Stevens
 * @author Joey Silva
 */
public class PageRank implements MatrixRank {

    /**
     * The weight factor given to the random jump probabilities versus the
     * actual edge weights in the affinity matrix.
     */
    private final double randomJumpWeight;

    private final double affinityWeight;

    /**
     * Creates a new {@link PageRank}. 
     */
    public PageRank(double randomJumpWeight) {
        this.randomJumpWeight = randomJumpWeight;
        this.affinityWeight = 1 - randomJumpWeight;
    }

    /**
     * {@inherDoc}
     */
    public DoubleVector rankMatrix(SparseMatrix affinityMatrix, 
                                   DoubleVector initialRanks) {
        int numRows = affinityMatrix.rows(); 

        DoubleVector pageRanks = initialRanks;

        // Normalize the wieght of the outlinks for each node such that they
        // represent a probability distribution, i.e. they sum to 1.
        for(int r = 0; r < numRows; r++) {
            SparseDoubleVector row = affinityMatrix.getRowVector(i);

            // Sum the weights of the row.
            double rowSum = 0;
            for (int c : row.getNonZeroIndices())
                rowSum += row.get(c);

            // Normalize by the row sum if it is non-zero.
            if (rowSum == 0d)
                continue;
            for (int c : row.getNonZeroIndices())
                affinityMatrix.set(r, c, row.get(c) / rowSum);
        }

        // Iterate over a row, saving the intermediate sum of each row use
        // built-in fn to get sparse row and also getnonzero before summing
        // multiply by position in v at the end.
        for(int k = 0; k < 20; k++) {
            DoubleVector newPageRanks = new DenseVector(numRows);

            // Compute the ranks based on a random walk through the affinity
            // matrix.  Traditionally, this is done by multiplying the transpose
            // of the affinity matrix by the page rank vector.  Since computing
            // the transpose is costly, both memory wise and access wise, we
            // compute it without doing the transpose directly.
            for(int i = 0; i < numRows; i++) {
                SparseDoubleVector row = affinityMatrix.getRowVector(i);
                for(int j:row.getNonZeroIndices())
                    newPageRanks.add(j, row.get(j) * pageRanks.get(i));
            }

            // Scale each computed rank by the factor given to the random jump
            // weight. 
            for(int i = 0; i < numRows; i++)
                newPageRanks.set(i, newPageRanks.get(i) * affinityWeight);

            // Compute the difference between the l1 norms of the old page ranks
            // and the new ranks.
            double pageRanksSum = 0; 
            double newPageRanksSum = 0;
            for(int i = 0; i < numRows; i++) {
                pageRanksSum += Math.abs(pageRanks.get(i));
                newPageRanksSum += Math.abs(newPageRanks.get(i));
            }

            // Add in the weight given to the random jump probabilities.
            double pageRankDifference =
                randomJumpWeight * (pageRanksSum - newPageRanksSum);
            for(int i = 0; i < numRows; i++) 
                newPageRanks.add(i, pageRankDifference * initialRanks.get(i));

            // Store the new page rank scores.
            pageRanks = newPageRanks;
        }

        return pageRanks;
    }

    /**
     * {@inherDoc}
     */
    public DoubleVector defaultRanks(SparseMatrix affinityMatrix) {
        DoubleVector initialRanks = new DenseVector(affinityMatrix.rows());
        for(int i = 0; i < affinityMatrix.rows(); i++) 
            initialRanks.set(i, 1d/numRows);
        return initialRanks;
    }
}
